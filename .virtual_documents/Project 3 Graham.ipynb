# Imports
import praw
import pandas as pd
import numpy as np 
import matplotlib.pyplot as plt
from dotenv import load_dotenv
import os
from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier
import joblib
from sklearn.svm import SVC
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report
from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS # From ChatGPT on how to get stopwords out after fitting the model
import re # from ChatGPT needed to remove numbers and symbols


np.random.seed(42)











# Commented out as I am not collecting any more data

# # Load the environment variables from the .env file
# load_dotenv('Reddit Info.env')

# # Access the env files
# secret_key = os.getenv('secret_key')
# reddit_username = os.getenv('reddit_username')
# reddit_password = os.getenv('reddit_password')
# personal_use_key = os.getenv('personal_use_key')





# Commented out as I am not collecting any more data

# reddit = praw.Reddit(
#     client_id= personal_use_key,
#     client_secret = secret_key,
#     password= reddit_password,
#     user_agent = "Class Project 3 (by u/glittering-pack-9564)",
#     username = reddit_username,
# )

# # checking to make sure it worked
# # should be false
# print(reddit.read_only)





# Commented out as I am not collecting any more data

# # Choosing subreddit and getting posts
# subreddit_wallstreetbets = reddit.subreddit('wallstreetbets')
# subreddit_stocks = reddit.subreddit('stocks')
# posts_wallstreetbets = subreddit_wallstreetbets.new(limit=100)
# posts_stocks = subreddit_stocks.new(limit=100)


# data_wallstreetbets = []
# for post in posts_wallstreetbets:
#     data_wallstreetbets.append([post.created_utc, post.title, post.selftext, post.subreddit])
    
# data_stocks = []
# for post in posts_stocks:
#     data_stocks.append([post.created_utc, post.title, post.selftext, post.subreddit])

# # Turn into a dataframe
# wallstreetbets = pd.DataFrame(data_wallstreetbets, columns = ['created_utc', 'title', 'self_text', 'subreddit'])
# stocks = pd.DataFrame(data_stocks, columns = ['created_utc', 'title', 'self_text', 'subreddit'])

# # Drop any duplicate posts
# wallstreetbets = wallstreetbets.drop_duplicates(subset=['created_utc', 'title'])
# stocks = stocks.drop_duplicates(subset=['created_utc', 'title'])






# Commented out as I am not collecting any more data

# # saves data to csv and appends new data
# wallstreetbets.to_csv('./data/Wallstreetbets data.csv',
#                      mode = 'a', # appending to the .csv file
#                      index = False,
#                      header = False) # taking off the headers for easier appending

# stocks.to_csv('./data/stocks data.csv',
#                      mode = 'a',
#                      index = False,
#                      header = False)

# gets back all the data with the new posts
wallstreetbets = pd.read_csv('./data/Wallstreetbets data.csv',names=['created_utc', 'title', 'self_text', 'subreddit'])
stocks = pd.read_csv('./data/Stocks data.csv',names = ['created_utc', 'title', 'self_text', 'subreddit'])


wallstreetbets.shape


wallstreetbets.head()


stocks.head()


stocks.shape


wallstreetbets.isnull().sum()


stocks.isnull().sum()





# I know that the subreddit wallstreetbets sometimes will post images or videos with no text and only a title so I will replace NaNs with just a word
# I do not think the stocks subreddit is allowed to do that due to the rules of the subreddit but it cannot hurt.
wallstreetbets['self_text'] = wallstreetbets['self_text'].replace({np.nan:'Image'})
stocks['self_text'] = stocks['self_text'].replace({np.nan:'Image'})





df_model_1 = pd.concat([wallstreetbets, stocks],
                      ignore_index = True) # the index does not matter 
df_model_1['subreddit'] = df_model_1['subreddit'].map({'wallstreetbets': 0,
                                         'stocks': 1})


df_model_1.head()





# commenting out because the model was saved 

X = df_model_1[['title','self_text']]
y = df_model_1['subreddit']
X_train, X_test, y_train, y_test = train_test_split(X, y)

# pipe_1 = Pipeline([
#     ('cvec',CountVectorizer()),
#     ('rf', RandomForestClassifier())
# ])

# pipe_params_1 = {
#     'cvec__max_features': np.arange(400,551,10), # Max number of frequently occuring words to include
#     'cvec__min_df': np.arange(1,11),  # Lowest amount of times a word can appear to be included
#     'cvec__max_df': [0.8, 0.95], # exclude words that appear in 80% or more of the posts
#     'cvec__ngram_range': [(1,1), (1,2), (1,3)], # n-grams uni, bi, tri
#     'rf__n_estimators': np.arange(50,250),  # Number of trees in the forest
#     'rf__max_depth': np.arange(1,31),  # Maximum depth of the tree
#     'rf__min_samples_split': [2,3,4],  # Minimum number of samples to split a node
# }

# rs_1 = RandomizedSearchCV(
#     pipe_1, 
#     pipe_params_1,
#     n_iter = 50,
#     cv=5,
#     n_jobs = -1 # use all available cores
# )


# Best Parameters: {'rf__n_estimators': 113, 'rf__min_samples_split': 3, 'rf__max_depth': 30, 'cvec__ngram_range': (1, 3), 'cvec__min_df': 6, 'cvec__max_features': 410, 'cvec__max_df': 0.8}
# Best Cross-Validation Score: 0.9848484848484848
# Training Accuracy: 0.9981818181818182
# Test Accuracy: 0.9818181818181818


# %%time
# rs_1.fit(X_train['title'] + " " + X_train['self_text'], y_train)


# # save the model
# # Credit to chatgpt for showing me this
# joblib.dump(rs_1,'./Models/Random Forest Model 1.pkl')

# Reload model
# Changed name of model to rf_1 from rs_1
rf_1 = joblib.load('./Models/Random Forest Model 1.pkl')


print('Best Parameters:', rf_1.best_params_)
print('Best Cross-Validation Score:', rf_1.best_score_)
print('Training Accuracy:', rf_1.score(X_train['title'] + " " + X_train['self_text'],y_train))
print('Test Accuracy:', rf_1.score(X_test['title'] + " " + X_test['self_text'],y_test))





ConfusionMatrixDisplay.from_estimator(rf_1,X_test['title'] + " " + X_test['self_text'],y_test, cmap = 'PuBu');


y_pred = rf_1.predict(X_test['title'] + " " + X_test['self_text'])

# Classification report
report = classification_report(y_test,y_pred)
# Print the report
print(report)





# Get the trained vectorizer from your pipeline
vectorizer = rf_1.best_estimator_.named_steps['cvec']

# Extract feature names from the CountVectorizer
feature_names = vectorizer.get_feature_names_out()

# Get the Random Forest classifier from the pipeline
model = rf_1.best_estimator_.named_steps['rf']

# Get feature importances from the Random Forest model
importances = model.feature_importances_

# Filter out stop words
non_stop_word_indices = [i for i, word in enumerate(feature_names) if word not in ENGLISH_STOP_WORDS]
filtered_features = feature_names[non_stop_word_indices]
filtered_importances = importances[non_stop_word_indices]


# Display top features without stop words
top_indices = np.argsort(filtered_importances)[-10:]
top_features = filtered_features[top_indices]
top_importances = filtered_importances[top_indices]


plt.barh(top_features, top_importances)
plt.xlabel("Feature Importance")
plt.title("Top Features Without Stop Words")
plt.show()


# Concatenate all text for each subreddit
wallstreetbets_text = re.sub(r"[^a-zA-Z0-9\s]", "", # ChatGPT gave me this re.sub part
    " ".join(df_model_1[df_model_1['subreddit'] == 0]['title'] + " " +
             df_model_1[df_model_1['subreddit'] == 0]['self_text']).lower())

stocks_text = re.sub(r"[^a-zA-Z0-9\s]", "", 
    " ".join(df_model_1[df_model_1['subreddit'] == 1]['title'] + " " +
             df_model_1[df_model_1['subreddit'] == 1]['self_text']).lower())

# Count word frequencies
wallstreetbets_counts = pd.Series(wallstreetbets_text.split()).value_counts()
stocks_counts = pd.Series(stocks_text.split()).value_counts()

# Filter out stop words
wallstreetbets_filtered_counts = wallstreetbets_counts[~wallstreetbets_counts.index.isin(ENGLISH_STOP_WORDS)]
stocks_filtered_counts = stocks_counts[~stocks_counts.index.isin(ENGLISH_STOP_WORDS)]

# Get top 10 most frequent words for each subreddit
top_wallstreetbets_words = wallstreetbets_filtered_counts.head(10)
top_stocks_words = stocks_filtered_counts.head(10)

# Plot for WallStreetBets
plt.barh(top_wallstreetbets_words.index, top_wallstreetbets_words.values)
plt.title('Top 10 Words in WallStreetBets Without Stop Words')
plt.xlabel('Frequency')

plt.tight_layout()
plt.show()

plt.savefig('./Images/Top 10 Words in Wallstreetbets (Stop Words Removed)')


# Plot for Stocks
plt.barh(top_stocks_words.index, top_stocks_words.values, color='orange')
plt.title('Top 10 Words in Stocks Without Stop Words')
plt.xlabel('Frequency')
plt.savefig('./Images/Top 10 Words in Stocks (Stop Words Removed)')





# commenting out because the model was saved

# X = df_model_1[['title','self_text']]
# y = df_model_1['subreddit']
# X_train, X_test, y_train, y_test = train_test_split(X, y)

# pipe_2 = Pipeline([
#     ('cvec',CountVectorizer()),
#     ('svm', SVC(kernel = 'rbf'))
# ])

# pipe_params_2 = {
#     'cvec__max_features': [500,700,1000], # Max number of frequently occuring words to include
#     'cvec__min_df': [5,15,20,30],  # Lowest amount of times a word can appear to be included
#     'cvec__max_df': [0.8, 0.95], # exclude words that appear in 80% or more of the posts
#     'cvec__ngram_range': [(1,1), (1,2), (1,3)], # n-grams uni, bi, tr,
#     'svm__C': np.linspace(0.001,2,10),  # Regularization parameter
# }

# rs_2 = RandomizedSearchCV(
#     pipe_2, 
#     pipe_params_2,
#     n_iter = 500,
#     cv=5,
#     n_jobs = -1 # use all available cores
# )


# Best Parameters: {'svm__C': 2.0, 'cvec__ngram_range': (1, 1), 'cvec__min_df': 15, 'cvec__max_features': 1000, 'cvec__max_df': 0.95}
# Best Cross-Validation Score: 0.9745454545454544
# Training Accuracy: 0.990909090909091
# Test Accuracy: 0.9654545454545455


# %%time
# rs_2.fit(X_train['title'] + " " + X_train['self_text'], y_train)

# save the model
# joblib.dump(rs_2,'./Models/Support Vector Machine Model.pkl')

# Reload model
svm_1 = joblib.load('./Models/Support Vector Machine Model.pkl')


print('Best Parameters:', svm_1.best_params_)
print('Best Cross-Validation Score:', svm_1.best_score_)
print('Training Accuracy:', svm_1.score(X_train['title'] + " " + X_train['self_text'],y_train))
print('Test Accuracy:', svm_1.score(X_test['title'] + " " + X_test['self_text'],y_test))



