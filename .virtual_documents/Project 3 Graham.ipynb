# Imports
import praw
import pandas as pd
import numpy as np 
import matplotlib.pyplot as plt
from dotenv import load_dotenv
import os
from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier
import joblib
from sklearn.svm import SVC


np.random.seed(42)











# Load the environment variables from the .env file
load_dotenv('Reddit Info.env')

# Access the env files
secret_key = os.getenv('secret_key')
reddit_username = os.getenv('reddit_username')
reddit_password = os.getenv('reddit_password')
personal_use_key = os.getenv('personal_use_key')





reddit = praw.Reddit(
    client_id= personal_use_key,
    client_secret = secret_key,
    password= reddit_password,
    user_agent = "Class Project 3 (by u/glittering-pack-9564)",
    username = reddit_username,
)

# checking to make sure it worked
# should be false
print(reddit.read_only)





# Choosing subreddit and getting posts
subreddit_wallstreetbets = reddit.subreddit('wallstreetbets')
subreddit_stocks = reddit.subreddit('stocks')
posts_wallstreetbets = subreddit_wallstreetbets.new(limit=100)
posts_stocks = subreddit_stocks.new(limit=100)


data_wallstreetbets = []
for post in posts_wallstreetbets:
    data_wallstreetbets.append([post.created_utc, post.title, post.selftext, post.subreddit])
    
data_stocks = []
for post in posts_stocks:
    data_stocks.append([post.created_utc, post.title, post.selftext, post.subreddit])

# Turn into a dataframe
wallstreetbets = pd.DataFrame(data_wallstreetbets, columns = ['created_utc', 'title', 'self_text', 'subreddit'])
stocks = pd.DataFrame(data_stocks, columns = ['created_utc', 'title', 'self_text', 'subreddit'])

# Drop any duplicate posts
wallstreetbets = wallstreetbets.drop_duplicates(subset=['created_utc', 'title'])
stocks = stocks.drop_duplicates(subset=['created_utc', 'title'])






# saves data to csv and appends new data
wallstreetbets.to_csv('./data/Wallstreetbets data.csv',
                     mode = 'a', # appending to the .csv file
                     index = False,
                     header = False) # taking off the headers for easier appending

stocks.to_csv('./data/stocks data.csv',
                     mode = 'a',
                     index = False,
                     header = False)

# gets back all the data with the new posts
wallstreetbets = pd.read_csv('./data/Wallstreetbets data.csv',names=['created_utc', 'title', 'self_text', 'subreddit'])
stocks = pd.read_csv('./data/Stocks data.csv',names = ['created_utc', 'title', 'self_text', 'subreddit'])


wallstreetbets.shape


wallstreetbets.head()


stocks.head()


stocks.shape


wallstreetbets.isnull().sum()


stocks.isnull().sum()





# I know that the subreddit wallstreetbets sometimes will post images or videos with no text and only a title so I will replace NaNs with just a word
# I do not think the stocks subreddit is allowed to do that due to the rules of the subreddit but it cannot hurt.
wallstreetbets['self_text'] = wallstreetbets['self_text'].replace({np.nan:'Image'})
stocks['self_text'] = stocks['self_text'].replace({np.nan:'Image'})





df_model_1 = pd.concat([wallstreetbets, stocks],
                      ignore_index = True) # the index does not matter 
df_model_1['subreddit'] = df_model_1['subreddit'].map({'wallstreetbets': 0,
                                         'stocks': 1})





# commenting out because the model was saved 

# X = df_model_1[['title','self_text']]
# y = df_model_1['subreddit']
# X_train, X_test, y_train, y_test = train_test_split(X, y)

# pipe_1 = Pipeline([
#     ('cvec',CountVectorizer()),
#     ('rf', RandomForestClassifier())
# ])

# pipe_params_1 = {
#     'cvec__max_features': np.arange(400,551,10), # Max number of frequently occuring words to include
#     'cvec__min_df': np.arange(1,11),  # Lowest amount of times a word can appear to be included
#     'cvec__max_df': [0.8, 0.95], # exclude words that appear in 80% or more of the posts
#     'cvec__ngram_range': [(1,1), (1,2), (1,3)], # n-grams uni, bi, tri
#     'rf__n_estimators': np.arange(50,250),  # Number of trees in the forest
#     'rf__max_depth': np.arange(1,31),  # Maximum depth of the tree
#     'rf__min_samples_split': [2,3,4],  # Minimum number of samples to split a node
# }

# rs_1 = RandomizedSearchCV(
#     pipe_1, 
#     pipe_params_1,
#     n_iter = 50,
#     cv=5,
#     n_jobs = -1 # use all available cores
# )


# Best Parameters: {'rf__n_estimators': 61, 'rf__min_samples_split': 4, 'rf__max_depth': 20, 'cvec__ngram_range': (1, 1), 'cvec__min_df': 7, 'cvec__max_features': 470, 'cvec__max_df': 0.8}
# Best Cross-Validation Score: 0.9814814814814815
# Training Accuracy: 0.9985185185185185
# Test Accuracy: 0.9888888888888889


# commenting out because the model was saved

# %%time
# rs_1.fit(X_train['title'] + " " + X_train['self_text'], y_train)


# # save the model
# # Credit to chatgpt for showing me this
# joblib.dump(rs_1,'./Models/Random Forest Model 1.pkl')

# Reload model
rf_1 = joblib.load('./Models/Random Forest Model 1.pkl')


print('Best Parameters:', rf_1.best_params_)
print('Best Cross-Validation Score:', rf_1.best_score_)
print('Training Accuracy:', rf_1.score(X_train['title'] + " " + X_train['self_text'],y_train))
print('Test Accuracy:', rf_1.score(X_test['title'] + " " + X_test['self_text'],y_test))





# commenting out because the model was saved

# X = df_model_1[['title','self_text']]
# y = df_model_1['subreddit']
# X_train, X_test, y_train, y_test = train_test_split(X, y)

# pipe_2 = Pipeline([
#     ('cvec',CountVectorizer()),
#     ('svm', SVC(kernel = 'rbf'))
# ])

# pipe_params_2 = {
#     'cvec__max_features': [500,700,1000], # Max number of frequently occuring words to include
#     'cvec__min_df': [5,15,20,30],  # Lowest amount of times a word can appear to be included
#     'cvec__max_df': [0.8, 0.95], # exclude words that appear in 80% or more of the posts
#     'cvec__ngram_range': [(1,1), (1,2), (1,3)], # n-grams uni, bi, tr,
#     'svm__C': np.linspace(0.001,2,10),  # Regularization parameter
# }

# rs_2 = RandomizedSearchCV(
#     pipe_2, 
#     pipe_params_2,
#     n_iter = 500,
#     cv=5,
#     n_jobs = -1 # use all available cores
# )


# Best Parameters: {'svm__C': 2.0, 'cvec__ngram_range': (1, 1), 'cvec__min_df': 15, 'cvec__max_features': 1000, 'cvec__max_df': 0.95}
# Best Cross-Validation Score: 0.9726666666666667
# Training Accuracy: 0.9913333333333333
# Test Accuracy: 0.974


# commented out because the model was saved

# %%time
# rs_2.fit(X_train['title'] + " " + X_train['self_text'], y_train)

# # # save the model
# joblib.dump(rs_2,'./Models/Support Vector Machine Model.pkl')

# Reload model
svm_1 = joblib.load('./Models/Support Vector Machine Model.pkl')


print('Best Parameters:', svm_1.best_params_)
print('Best Cross-Validation Score:', svm_1.best_score_)
print('Training Accuracy:', svm_1.score(X_train['title'] + " " + X_train['self_text'],y_train))
print('Test Accuracy:', svm_1.score(X_test['title'] + " " + X_test['self_text'],y_test))



