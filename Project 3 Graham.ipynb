{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "623fc10d-3881-4562-9dab-5a9a6cb0942d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import praw\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "import joblib\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc9680c8-8526-4382-a98c-42c6dd36eec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3076da8a-b310-4868-be56-9b70e86adea9",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "1. [EDA](#eda)\n",
    "2. [Model 1: Random Forest with RandomizedSearchCV](#model-1-random-forest-with-randomizedsearchcv)\n",
    "3. [Model 2: Support Vector Machine with RandomizedSearchCV](#model-2-support-vector-machine-with-randomizedsearchcv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56189f0b-c119-476a-9a5d-cb156842b486",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71d2292-0758-400e-b7bc-afadd8d86ca8",
   "metadata": {},
   "source": [
    "Load in from the .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78ff5325-3d18-432d-b0d0-3d9b74d7b363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the environment variables from the .env file\n",
    "load_dotenv('Reddit Info.env')\n",
    "\n",
    "# Access the env files\n",
    "secret_key = os.getenv('secret_key')\n",
    "reddit_username = os.getenv('reddit_username')\n",
    "reddit_password = os.getenv('reddit_password')\n",
    "personal_use_key = os.getenv('personal_use_key')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24eab5f0-acea-4e3a-91bb-dd55a0387c3e",
   "metadata": {},
   "source": [
    "Access reddit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0c40dce-db14-4f84-a096-14af4d285f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "reddit = praw.Reddit(\n",
    "    client_id= personal_use_key,\n",
    "    client_secret = secret_key,\n",
    "    password= reddit_password,\n",
    "    user_agent = \"Class Project 3 (by u/glittering-pack-9564)\",\n",
    "    username = reddit_username,\n",
    ")\n",
    "\n",
    "# checking to make sure it worked\n",
    "# should be false\n",
    "print(reddit.read_only)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559cac68-3f0d-43a8-9632-f88a5224bffe",
   "metadata": {},
   "source": [
    "Choose two subreddits and get posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c125eba1-e90d-4fc5-9ed0-a54eca53abd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choosing subreddit and getting posts\n",
    "subreddit_wallstreetbets = reddit.subreddit('wallstreetbets')\n",
    "subreddit_stocks = reddit.subreddit('stocks')\n",
    "posts_wallstreetbets = subreddit_wallstreetbets.new(limit=100)\n",
    "posts_stocks = subreddit_stocks.new(limit=100)\n",
    "\n",
    "\n",
    "data_wallstreetbets = []\n",
    "for post in posts_wallstreetbets:\n",
    "    data_wallstreetbets.append([post.created_utc, post.title, post.selftext, post.subreddit])\n",
    "    \n",
    "data_stocks = []\n",
    "for post in posts_stocks:\n",
    "    data_stocks.append([post.created_utc, post.title, post.selftext, post.subreddit])\n",
    "\n",
    "# Turn into a dataframe\n",
    "wallstreetbets = pd.DataFrame(data_wallstreetbets, columns = ['created_utc', 'title', 'self_text', 'subreddit'])\n",
    "stocks = pd.DataFrame(data_stocks, columns = ['created_utc', 'title', 'self_text', 'subreddit'])\n",
    "\n",
    "# Drop any duplicate posts\n",
    "wallstreetbets = wallstreetbets.drop_duplicates(subset=['created_utc', 'title'])\n",
    "stocks = stocks.drop_duplicates(subset=['created_utc', 'title'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91c3e31-5464-44fc-bebc-0a847210ade9",
   "metadata": {},
   "source": [
    "Append posts to a .csv file in order to save and get back a dataframe from the .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "107117d0-7a7e-418a-a8ef-d079f5f0d99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saves data to csv and appends new data\n",
    "wallstreetbets.to_csv('./data/Wallstreetbets data.csv',\n",
    "                     mode = 'a', # appending to the .csv file\n",
    "                     index = False,\n",
    "                     header = False) # taking off the headers for easier appending\n",
    "\n",
    "stocks.to_csv('./data/stocks data.csv',\n",
    "                     mode = 'a',\n",
    "                     index = False,\n",
    "                     header = False)\n",
    "\n",
    "# gets back all the data with the new posts\n",
    "wallstreetbets = pd.read_csv('./data/Wallstreetbets data.csv',names=['created_utc', 'title', 'self_text', 'subreddit'])\n",
    "stocks = pd.read_csv('./data/Stocks data.csv',names = ['created_utc', 'title', 'self_text', 'subreddit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c53226d6-bdb8-43d2-a843-96f0e4a6a1fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1100, 4)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wallstreetbets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "351e0299-7572-4390-a597-ccdff36e43bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_utc</th>\n",
       "      <th>title</th>\n",
       "      <th>self_text</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.731948e+09</td>\n",
       "      <td>Bear Market due to bad NVIDIA Earning?</td>\n",
       "      <td>Reasoning:\\n\\n1. Due to the rising yields the ...</td>\n",
       "      <td>wallstreetbets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.731948e+09</td>\n",
       "      <td>Doing the inverse from now on</td>\n",
       "      <td>How to make a lose-lose situation. Exhibit A\\n...</td>\n",
       "      <td>wallstreetbets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.731947e+09</td>\n",
       "      <td>Wen moon?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>wallstreetbets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.731947e+09</td>\n",
       "      <td>Me forgetting NVDA earnings is this week and q...</td>\n",
       "      <td>I think I’m done.</td>\n",
       "      <td>wallstreetbets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.731946e+09</td>\n",
       "      <td>TIL Peloton is not up to 200% in a year and va...</td>\n",
       "      <td>Every now and then I check my little trading a...</td>\n",
       "      <td>wallstreetbets</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    created_utc                                              title  \\\n",
       "0  1.731948e+09             Bear Market due to bad NVIDIA Earning?   \n",
       "1  1.731948e+09                      Doing the inverse from now on   \n",
       "2  1.731947e+09                                          Wen moon?   \n",
       "3  1.731947e+09  Me forgetting NVDA earnings is this week and q...   \n",
       "4  1.731946e+09  TIL Peloton is not up to 200% in a year and va...   \n",
       "\n",
       "                                           self_text       subreddit  \n",
       "0  Reasoning:\\n\\n1. Due to the rising yields the ...  wallstreetbets  \n",
       "1  How to make a lose-lose situation. Exhibit A\\n...  wallstreetbets  \n",
       "2                                                NaN  wallstreetbets  \n",
       "3                                 I think I’m done.   wallstreetbets  \n",
       "4  Every now and then I check my little trading a...  wallstreetbets  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wallstreetbets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc777225-bd3c-49ae-8a61-b6d7b9294743",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_utc</th>\n",
       "      <th>title</th>\n",
       "      <th>self_text</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.731948e+09</td>\n",
       "      <td>EQIX and DLR - are they benefitting at all fro...</td>\n",
       "      <td>I had been wondering if datacenter REIT's such...</td>\n",
       "      <td>stocks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.731947e+09</td>\n",
       "      <td>What are some high risk, high reward stocks I ...</td>\n",
       "      <td>I'm still in high school, so I don't pay any e...</td>\n",
       "      <td>stocks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.731947e+09</td>\n",
       "      <td>Creating Stop Loss on Webull?</td>\n",
       "      <td>I'm trying to create my first stop Loss on Web...</td>\n",
       "      <td>stocks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.731945e+09</td>\n",
       "      <td>S&amp;P 500 Rebalancing Trade</td>\n",
       "      <td>The S&amp;P 500 index rebalancing occurs 4x/year w...</td>\n",
       "      <td>stocks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.731945e+09</td>\n",
       "      <td>BioNTech Analysis and Questions</td>\n",
       "      <td>Let me first talk about the BioNTech's Q3 2024...</td>\n",
       "      <td>stocks</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    created_utc                                              title  \\\n",
       "0  1.731948e+09  EQIX and DLR - are they benefitting at all fro...   \n",
       "1  1.731947e+09  What are some high risk, high reward stocks I ...   \n",
       "2  1.731947e+09                     Creating Stop Loss on Webull?    \n",
       "3  1.731945e+09                          S&P 500 Rebalancing Trade   \n",
       "4  1.731945e+09                   BioNTech Analysis and Questions    \n",
       "\n",
       "                                           self_text subreddit  \n",
       "0  I had been wondering if datacenter REIT's such...    stocks  \n",
       "1  I'm still in high school, so I don't pay any e...    stocks  \n",
       "2  I'm trying to create my first stop Loss on Web...    stocks  \n",
       "3  The S&P 500 index rebalancing occurs 4x/year w...    stocks  \n",
       "4  Let me first talk about the BioNTech's Q3 2024...    stocks  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stocks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96c1e83e-7bbe-49b5-bef7-b8f762b41bb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1100, 4)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stocks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3707a32-1052-428b-9b3e-66e1df36f777",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "created_utc      0\n",
       "title            0\n",
       "self_text      327\n",
       "subreddit        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wallstreetbets.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ff0d5ad-a8c8-455a-aaa9-9804f49cb1f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "created_utc    0\n",
       "title          0\n",
       "self_text      0\n",
       "subreddit      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stocks.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06b4287-319e-452b-a359-e20ac0bcfd16",
   "metadata": {},
   "source": [
    "Get rid of any null values for both DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e8b4e01-3258-402c-9dc5-7afec0d90b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I know that the subreddit wallstreetbets sometimes will post images or videos with no text and only a title so I will replace NaNs with just a word\n",
    "# I do not think the stocks subreddit is allowed to do that due to the rules of the subreddit but it cannot hurt.\n",
    "wallstreetbets['self_text'] = wallstreetbets['self_text'].replace({np.nan:'Image'})\n",
    "stocks['self_text'] = stocks['self_text'].replace({np.nan:'Image'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241f2be8-3443-4b61-b642-569f5a0e9731",
   "metadata": {},
   "source": [
    "Concatenate both dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "efa21f38-e0d3-45ab-9a4d-b23f8e3ac51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_1 = pd.concat([wallstreetbets, stocks],\n",
    "                      ignore_index = True) # the index does not matter \n",
    "df_model_1['subreddit'] = df_model_1['subreddit'].map({'wallstreetbets': 0,\n",
    "                                         'stocks': 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e35d6f-cea3-4c65-ac46-85cdc7f397bc",
   "metadata": {},
   "source": [
    "## Model 1: Random Forest with RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a8acdc2e-7851-4818-888e-d358a8b1cb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# commenting out because the model was saved \n",
    "\n",
    "# X = df_model_1[['title','self_text']]\n",
    "# y = df_model_1['subreddit']\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "# pipe_1 = Pipeline([\n",
    "#     ('cvec',CountVectorizer()),\n",
    "#     ('rf', RandomForestClassifier())\n",
    "# ])\n",
    "\n",
    "# pipe_params_1 = {\n",
    "#     'cvec__max_features': np.arange(400,551,10), # Max number of frequently occuring words to include\n",
    "#     'cvec__min_df': np.arange(1,11),  # Lowest amount of times a word can appear to be included\n",
    "#     'cvec__max_df': [0.8, 0.95], # exclude words that appear in 80% or more of the posts\n",
    "#     'cvec__ngram_range': [(1,1), (1,2), (1,3)], # n-grams uni, bi, tri\n",
    "#     'rf__n_estimators': np.arange(50,250),  # Number of trees in the forest\n",
    "#     'rf__max_depth': np.arange(1,31),  # Maximum depth of the tree\n",
    "#     'rf__min_samples_split': [2,3,4],  # Minimum number of samples to split a node\n",
    "# }\n",
    "\n",
    "# rs_1 = RandomizedSearchCV(\n",
    "#     pipe_1, \n",
    "#     pipe_params_1,\n",
    "#     n_iter = 50,\n",
    "#     cv=5,\n",
    "#     n_jobs = -1 # use all available cores\n",
    "# )\n",
    "\n",
    "\n",
    "# Best Parameters: {'rf__n_estimators': 113, 'rf__min_samples_split': 3, 'rf__max_depth': 30, 'cvec__ngram_range': (1, 3), 'cvec__min_df': 6, 'cvec__max_features': 410, 'cvec__max_df': 0.8}\n",
    "# Best Cross-Validation Score: 0.9848484848484848\n",
    "# Training Accuracy: 0.9981818181818182\n",
    "# Test Accuracy: 0.9818181818181818"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "87221d88-c352-4131-bd31-fb785635c343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 2.91 s\n",
      "Wall time: 26 s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# rs_1.fit(X_train['title'] + \" \" + X_train['self_text'], y_train)\n",
    "\n",
    "\n",
    "# # save the model\n",
    "# # Credit to chatgpt for showing me this\n",
    "# joblib.dump(rs_1,'./Models/Random Forest Model 1.pkl')\n",
    "\n",
    "# Reload model\n",
    "rf_1 = joblib.load('./Models/Random Forest Model 1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a86e080e-8b37-41a6-b4b3-e7f13c431e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'rf__n_estimators': 113, 'rf__min_samples_split': 3, 'rf__max_depth': 30, 'cvec__ngram_range': (1, 3), 'cvec__min_df': 6, 'cvec__max_features': 410, 'cvec__max_df': 0.8}\n",
      "Best Cross-Validation Score: 0.9848484848484848\n",
      "Training Accuracy: 0.9981818181818182\n",
      "Test Accuracy: 0.9818181818181818\n"
     ]
    }
   ],
   "source": [
    "print('Best Parameters:', rf_1.best_params_)\n",
    "print('Best Cross-Validation Score:', rf_1.best_score_)\n",
    "print('Training Accuracy:', rf_1.score(X_train['title'] + \" \" + X_train['self_text'],y_train))\n",
    "print('Test Accuracy:', rf_1.score(X_test['title'] + \" \" + X_test['self_text'],y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ff6ae4-e1f9-4b2b-80ae-8c8aa86d3bbb",
   "metadata": {},
   "source": [
    "## Model 2: Support Vector Machine with RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "863bd27b-ccf3-4378-a032-e6759a652611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# commenting out because the model was saved\n",
    "\n",
    "# X = df_model_1[['title','self_text']]\n",
    "# y = df_model_1['subreddit']\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "# pipe_2 = Pipeline([\n",
    "#     ('cvec',CountVectorizer()),\n",
    "#     ('svm', SVC(kernel = 'rbf'))\n",
    "# ])\n",
    "\n",
    "# pipe_params_2 = {\n",
    "#     'cvec__max_features': [500,700,1000], # Max number of frequently occuring words to include\n",
    "#     'cvec__min_df': [5,15,20,30],  # Lowest amount of times a word can appear to be included\n",
    "#     'cvec__max_df': [0.8, 0.95], # exclude words that appear in 80% or more of the posts\n",
    "#     'cvec__ngram_range': [(1,1), (1,2), (1,3)], # n-grams uni, bi, tr,\n",
    "#     'svm__C': np.linspace(0.001,2,10),  # Regularization parameter\n",
    "# }\n",
    "\n",
    "# rs_2 = RandomizedSearchCV(\n",
    "#     pipe_2, \n",
    "#     pipe_params_2,\n",
    "#     n_iter = 500,\n",
    "#     cv=5,\n",
    "#     n_jobs = -1 # use all available cores\n",
    "# )\n",
    "\n",
    "\n",
    "# Best Parameters: {'svm__C': 2.0, 'cvec__ngram_range': (1, 1), 'cvec__min_df': 15, 'cvec__max_features': 1000, 'cvec__max_df': 0.95}\n",
    "# Best Cross-Validation Score: 0.9745454545454544\n",
    "# Training Accuracy: 0.990909090909091\n",
    "# Test Accuracy: 0.9654545454545455"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3743de1b-25af-4200-bc53-0ae5c03501d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 16.7 s\n",
      "Wall time: 3min 38s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# rs_2.fit(X_train['title'] + \" \" + X_train['self_text'], y_train)\n",
    "\n",
    "# save the model\n",
    "# joblib.dump(rs_2,'./Models/Support Vector Machine Model.pkl')\n",
    "\n",
    "# Reload model\n",
    "svm_1 = joblib.load('./Models/Support Vector Machine Model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7ae26952-9ae0-40ce-a960-372d690dc774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'svm__C': 2.0, 'cvec__ngram_range': (1, 1), 'cvec__min_df': 15, 'cvec__max_features': 1000, 'cvec__max_df': 0.95}\n",
      "Best Cross-Validation Score: 0.9745454545454544\n",
      "Training Accuracy: 0.990909090909091\n",
      "Test Accuracy: 0.9654545454545455\n"
     ]
    }
   ],
   "source": [
    "print('Best Parameters:', svm_1.best_params_)\n",
    "print('Best Cross-Validation Score:', svm_1.best_score_)\n",
    "print('Training Accuracy:', svm_1.score(X_train['title'] + \" \" + X_train['self_text'],y_train))\n",
    "print('Test Accuracy:', svm_1.score(X_test['title'] + \" \" + X_test['self_text'],y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c555a3-7c7b-48c9-8289-48e0d31c79fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
